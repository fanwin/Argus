# Pytest Configuration File
# This file defines global configuration options for the pytest testing framework
# For more configuration options, see: https://docs.pytest.org/en/stable/reference/reference.html#configuration-options

[pytest]

# ==========================================
# Test Discovery Configuration
# ==========================================

# Test file search paths - pytest will recursively search for test files in these directories
testpaths = tests

# Python test file naming rules - .py files matching these patterns will be recognized as test files
python_files = test_*.py *_test.py

# Python test class naming rules - classes matching these patterns will be recognized as test classes
python_classes = Test*

# Python test function naming rules - functions matching these patterns will be recognized as test functions
python_functions = test_*

# ==========================================
# Output Configuration
# ==========================================

# Default command line options - these options will be automatically applied every time pytest is run
addopts =
    # -v: verbose mode, show detailed information for each test
    -v
    
    # --strict-markers: strict marker mode, undefined markers will cause errors
    --strict-markers
    
    # --strict-config: strict configuration mode, configuration errors will cause failures
    --strict-config
    
    # --tb=short: short traceback information format
    --tb=short
    
    # --html: generate HTML format test report
;    --html=reports/report.html
    
    # --self-contained-html: generate self-contained HTML report (includes CSS and JS)
    --self-contained-html
    
    # --cov: enable code coverage checking for current directory
    --cov=.
    
    # --cov-report=html: generate HTML format coverage report
    --cov-report=html:reports/coverage
    
    # --cov-report=term-missing: show uncovered lines in terminal
    --cov-report=term-missing
    
    # --alluredir: generate Allure test report data
    --alluredir=reports/allure-results
    
    # --reruns: number of retries for failed tests
    --reruns=1
    
    # --reruns-delay: delay time between retries (seconds)
    --reruns-delay=2

# ==========================================
# Markers Definition
# ==========================================

# Custom test markers for categorizing and filtering tests
# Usage: @pytest.mark.smoke decorator to mark tests
# Run examples:
#   pytest -m smoke                    # run only smoke tests
#   pytest -m "not slow"               # exclude slow tests
#   pytest -m "smoke and api"          # run tests that are both smoke and api
markers =
    # Smoke tests: quick verification of core functionality, usually run after each build
    smoke: Smoke tests - Quick verification of core functionality
    
    # Regression tests: tests to verify that fixes work correctly
    regression: Regression tests - Verify that fixes work correctly
    
    # API tests: tests for API endpoints and services
    api: API tests - Tests for API endpoints and services
    
    # Web UI tests: tests for web user interface
    web: Web UI tests - Tests for web user interface
    
    # Slow tests: tests that take a long time to execute (usually >30 seconds)
    slow: Slow tests - Tests that take a long time to execute
    
    # Integration tests: tests for component interactions
    integration: Integration tests - Tests for component interactions
    
    # Unit tests: tests for individual units of code
    unit: Unit tests - Tests for individual units of code
    
    # Critical tests: tests for core business features
    critical: Critical functionality tests - Tests for core business features
    
    # Security tests: tests for security features and vulnerabilities
    security: Security tests - Tests for security features and vulnerabilities
    
    # Development environment tests: tests specific to dev environment
    dev: Development environment tests - Tests specific to dev environment
    
    # Staging environment tests: tests specific to staging environment
    staging: Staging environment tests - Tests specific to staging environment
    
    # Production environment tests: tests specific to production environment
    prod: Production environment tests - Tests specific to production environment
    
    # Sample tests: example tests for demonstration
    sample: Sample tests - Example tests for demonstration

    # Distributed tests: tests designed for distributed execution
    distributed: Distributed tests - Tests designed for distributed execution

    # Flaky tests: tests that may fail intermittently
    flaky: Flaky tests - Tests that may fail intermittently

    # Accessibility tests: tests for web accessibility compliance
    accessibility: Accessibility tests - Tests for web accessibility compliance

    # WCAG tests: tests for WCAG (Web Content Accessibility Guidelines) compliance
    wcag: WCAG tests - Tests for WCAG compliance

    # Keyboard navigation tests: tests for keyboard accessibility
    keyboard: Keyboard navigation tests - Tests for keyboard accessibility

    # ARIA tests: tests for ARIA (Accessible Rich Internet Applications) attributes
    aria: ARIA tests - Tests for ARIA attributes

    # Comprehensive tests: comprehensive test suites
    comprehensive: Comprehensive tests - Comprehensive test suites

    # Performance tests: tests for performance benchmarks and load testing
    performance: Performance tests - Tests for performance benchmarks and load testing

    # Data-driven tests: tests using data-driven approach
    data_driven: Data-driven tests - Tests using data-driven approach

    # Timeout tests: tests with timeout constraints
    timeout: Timeout tests - Tests with timeout constraints

    # Cross-service tests: tests for cross-service integration
    cross_service: Cross-service tests - Tests for cross-service integration

    # Mobile tests: tests for mobile applications
    mobile: Mobile tests - Tests for mobile applications

# ==========================================
# Warning Filters
# ==========================================

# Warning handling rules
# ignore: ignore specified types of warnings
# error: convert warnings to errors
# default: use default warning handling
filterwarnings =
    # Ignore user warnings (usually from third-party libraries)
    ignore::UserWarning
    
    # Ignore deprecation warnings (avoid third-party library deprecation warnings interfering with tests)
    ignore::DeprecationWarning

# ==========================================
# Version Requirements
# ==========================================

# Minimum pytest version requirement - ensure the pytest version supports required features
minversion = 6.0

# ==========================================
# Logging Configuration
# ==========================================

# Console logging configuration
# Whether to display log output in console in real time
log_cli = true

# Console log level
# DEBUG: debug information (most detailed)
# INFO: general information
# WARNING: warning information
# ERROR: error information
# CRITICAL: critical errors (most concise)
log_cli_level = INFO

# Console log format
# %(asctime)s: timestamp
# %(levelname)8s: log level (8 character width, right-aligned)
# %(name)s: logger name (usually module name)
# %(message)s: log message content
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s

# Console log time format
log_cli_date_format = %Y-%m-%d %H:%M:%S

# File logging configuration
# Log file save path
log_file = reports/pytest.log

# File log level (usually more detailed than console)
log_file_level = DEBUG

# File log format (includes filename and line number for debugging)
# %(filename)s: source filename
# %(lineno)d: source code line number
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d: %(message)s

# File log time format
log_file_date_format = %Y-%m-%d %H:%M:%S

# ==========================================
# Parallel Testing Configuration
# ==========================================

# Parallel testing instructions:
# Use pytest-xdist plugin to run tests in parallel
# Command examples:
#   pytest -n auto                    # automatically detect CPU cores and run in parallel
#   pytest -n 4                      # use 4 processes to run in parallel
#   pytest -n auto --dist=loadscope  # distribute tests by scope
#
# Notes:
# 1. Parallel testing may cause resource competition, ensure tests are independent of each other
# 2. Database tests need to use different databases or transaction isolation
# 3. File operation tests need to use different temporary files
# 4. Parallel testing will increase system resource consumption
